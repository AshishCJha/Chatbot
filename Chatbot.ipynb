{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishCJha/Chatbot/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB7g1WF4r9S7",
        "colab_type": "code",
        "outputId": "71c196a6-094b-4232-ce0b-184dbe413a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsPDiBNW1naS",
        "colab_type": "code",
        "outputId": "da649652-6cae-4bef-be75-64ce23eb10d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/RNN\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " atom_text\t   'Language Translation'     movie_lines.txt\n",
            " cmudict-0.7b.txt   movie_conversations.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oCMiU1ms7U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHSYMQo12D2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers.core import Dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pdLutPb2qDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing movie conversation and lines\n",
        "lines = open('/content/drive/My Drive/Colab_Notebooks/Chatbot/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/drive/My Drive/Colab_Notebooks/Chatbot/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r40R-pr_F7Jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "26ae8a4a-5f7d-4067-9130-efd2159591c2"
      },
      "source": [
        "type(lines)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiP7pUZtGFM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_df = pd.DataFrame(lines)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix_ZTClEGinZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "66803ca9-691a-4c8c-931f-13868e7d2aa2"
      },
      "source": [
        "lines_df.head()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L19...\n",
              "1  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
              "2  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L20...\n",
              "3  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L20...\n",
              "4  u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzs6HSzLSekX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        " \n",
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        " \n",
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])\n",
        "    \n",
        "\n",
        "def clean_text(text):\n",
        "  \n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"you'r\", \"you are\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\" \\'m\", \" am\", text)  \n",
        "    text = re.sub(r\"\\'d\", \"would\", text)\n",
        "    text = re.sub(r\"they'r\", \"they are\", text)\n",
        "    text = re.sub(r\"\\'ve\", \"have\", text)\n",
        "    text = re.sub(r\"this's\", \"this is\", text)\n",
        "    text = re.sub(r\"'what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"who's\", \"who is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"\\'re\", \"are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"  \", \" \", text)\n",
        "    text = re.sub(r'\"' , \"\", text)\n",
        "    text = re.sub(r\"'\" , \"\", text)\n",
        "    text = re.sub(r\"[0-9]+\" , \"\", text)\n",
        "    text = re.sub(r\"<b>\" , \"\", text)\n",
        "    text = re.sub(r\"<i>\" , \"\", text)\n",
        "    text = re.sub(r\"<\" , \"\", text)\n",
        "    text = re.sub(r\">\" , \"\", text)  \n",
        "    text = re.sub(r\"[~`!@#$%^&*_=():;/?_+|,.-]\",\"\",text)\n",
        "    text=text.replace(\"[\",\"\")\n",
        "    text=text.replace(\"]\",\"\")\n",
        "    return text\n",
        "  \n",
        "def clean_data():\n",
        "  # Cleaning the questions\n",
        "  cleaned_questions = []\n",
        "  for question in questions:\n",
        "      cleaned_questions.append(clean_text(question))\n",
        "\n",
        "  # Cleaning the answers\n",
        "  cleaned_answers = []\n",
        "  for answer in answers:\n",
        "      cleaned_answers.append(clean_text(answer))\n",
        "  return cleaned_questions, cleaned_answers\n",
        "                                        \n",
        " #This function is used to create vocabulary, word_to_id and id_to_word dicts from cleaned data (got from the last question).   \n",
        "def create_vocab(questions, answers):\n",
        "\n",
        "\tassert len(questions) == len(answers)\n",
        "\tvocab = []\n",
        "\tfor i in range(len(questions)):\n",
        "\t\twords = questions[i].split()\n",
        "\t\tfor word in words:\n",
        "\t\t\tvocab.append(word)\n",
        "\n",
        "\t\twords = answers[i].split()\n",
        "\t\tfor word in words:\n",
        "\t\t\tvocab.append(word)\n",
        "\n",
        "\tvocab = Counter(vocab)\n",
        "\ttokens = []\n",
        "\tfor key in vocab.keys():\n",
        "\t\tif vocab[key] >= config_hp.Vocab_Threshold:\n",
        "\t\t\ttokens.append(key)\n",
        "\n",
        "\ttokens = ['<PAD>', '<SOS>', '<UNK>', '<EOS>'] + tokens\n",
        "\n",
        "\tword_to_id = {word:i for i, word in enumerate(tokens)}\n",
        "\tid_to_word = {i:word for i, word in enumerate(tokens)}\n",
        "\n",
        "\treturn tokens, word_to_id, id_to_word\n",
        "  \n",
        "#Using word_to_id dictionery to map each word in the sample to it's own int representation\n",
        "def encoder_word_to_id(data, word_to_id, targets=False):\n",
        "\n",
        "\tencoded_data = []\n",
        "\n",
        "\tfor i in range(len(data)):\n",
        "\n",
        "\t\tencoded_line = []\n",
        "\t\twords = data[i].split()\n",
        "\t\tfor word in words:\n",
        "\n",
        "\t\t\tif word not in word_to_id.keys():\n",
        "\t\t\t\tencoded_line.append(word_to_id['<UNK>'])\n",
        "\t\t\telse:\n",
        "\t\t\t\tencoded_line.append(word_to_id[word])\n",
        "\n",
        "\t\tif targets:\n",
        "\t\t\tencoded_line.append(word_to_id['<EOS>'])\n",
        "\n",
        "\t\tencoded_data.append(encoded_line)\n",
        "\n",
        "\n",
        "\treturn np.array(encoded_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-VSQhc9e5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up Hyperparameter\n",
        "config_hp = tf.contrib.training.HParams(Vocab_Threshold = 20,\n",
        "                                        Num_Layers =3,\n",
        "                                        Epochs = 150,\n",
        "                                        Batch_Size = 64,\n",
        "                                        Learning_Rate = 0.0001,\n",
        "                                        Learning_Rate_Decay = 0.9, \n",
        "                                        Min_Learning_Rate = 0.0001,\n",
        "                                        Keep_Prob = 0.5,\n",
        "                                        Clip_Rate=4,\n",
        "                                        Rnn_Size = 64,\n",
        "                                        Encod_Embed_Size= 64,\n",
        "                                        Decode_Embed_Size= 64,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W1Q1beySmbu",
        "colab_type": "text"
      },
      "source": [
        "############ MODEL FUNCTIONS ##################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLsYVlweSr8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#graph_inputs function is used to define all tensorflow graph placeholders\n",
        "\n",
        "\n",
        "def graph_inputs():\n",
        "  \n",
        "    #Inputs placeholder will be fed with question sentence data, and its shape is [None, None]\n",
        "    #The first None means the batch size, and the batch size is unknown since user can set it\n",
        "    #The second None means the lengths of sentences.\n",
        "    en_inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    \n",
        "    #en_targets placeholder is similar to inputs placeholder except that it will be fed with answer sentence data.\n",
        "    en_targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    \n",
        "    #keep_probs - probabilities used in DropoutWrapper in dropout layer(generally we are using it for generalization of the model)\n",
        "    keep_probs = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "    \n",
        "    #encoder_seq_len - vector placeholder represents the lengths of each sentences, so the shape is None\n",
        "    encoder_seq_len = tf.placeholder(tf.int32, (None, ), name='encoder_seq_len')\n",
        "    \n",
        "    #decoder_seq_len - vector which is used to define lengths of each sample in the targets to the model\n",
        "    decoder_seq_len = tf.placeholder(tf.int32, (None, ), name='decoder_seq_len')\n",
        "    \n",
        "    #max_seq_len - gets the maximum value out of lengths of all the target sentences(sequences)\n",
        "    max_seq_len = tf.reduce_max(decoder_seq_len, name='max_seq_len')\n",
        "    \n",
        "    return en_inputs, en_targets, keep_probs, encoder_seq_len, decoder_seq_len, max_seq_len\n",
        "\n",
        "\n",
        "def encoder_rnn_layer(inputs, rnn_size, number_of_layers, encoder_seq_len, keep_probs, encoder_embed_size, encoder_vocab_size):\n",
        "                \n",
        "    #rnn_size: int value, The number of units in the LSTM cell.\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "                \n",
        "    #An rnn_cell, a projection to output_size is added to it.\n",
        "    #keep_probs: unit Tensor or float between 0 and 1, if it is constant and 1, no input dropout will be added\n",
        "    rnn_cell= tf.contrib.rnn.DropoutWrapper(lstm, keep_probs)\n",
        "                \n",
        "    #encoder_cell: Composed sequentially of a number of rnn_cell.\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell] * number_of_layers)\n",
        "                \n",
        "    #encoder_embedding: Each word in a sentence will be represented with the number of features specified as encoder_embed_size\n",
        "    encoder_embedings = tf.contrib.layers.embed_sequence(inputs, encoder_vocab_size, encoder_embed_size) \n",
        "                \n",
        "    #Put embeding layer and rnn stacked layer all togather \n",
        "    encoder_outputs, encoder_states = tf.nn.dynamic_rnn(encoder_cell, encoder_embedings,encoder_seq_len,dtype=tf.float32)\n",
        "\n",
        "    return encoder_outputs, encoder_states\n",
        "\n",
        "def preprocessing_target(targets, word_to_id, batch_size):\n",
        "\t  #This line is used to REMOVE last member of each sample in the decoder_inputs batch\n",
        "    #Stride, we can think like we are splitting it with multiple stride window with some size of window   \n",
        "    endings = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])  \n",
        "    #tf.fill we can see we are creating tensor field with scaler value                        \n",
        "    #returning line and in this line we concat '<SOS>' tag at the beginning of each sample in the batch\n",
        "    decoder_inputs= tf.concat([tf.fill([batch_size, 1], word_to_id['<SOS>']), endings], 1) \n",
        "                \n",
        "    return decoder_inputs\n",
        "\n",
        "\n",
        "def decoder_rnn_layer(decoder_inputs, encoder_states, decoder_cell, decoder_embed_size, vocab_size, decoder_seq_len, max_seq_len, word_to_id, batch_size):\n",
        "\n",
        "    #Defining embedding layer for the Decoder, This is used to convert encode training target texts to list of ids.\n",
        "    embed_layer = tf.Variable(tf.random_uniform([vocab_size, decoder_embed_size]))\n",
        "    embeding_matrix = tf.nn.embedding_lookup(embed_layer, decoder_inputs) \n",
        "               \n",
        "    # Creating Dense (Fully Connected) layer at the end of the Decoder, a neural network operates on dense vectors of some size,\n",
        "    # often 256, 512 or 1024 floats (let's say 256 for here).But at the end it needs to predict a word from the vocabulary which is often much larger,    \n",
        "    # e.g., 40000 words. Output projection is the final linear layer that converts (projects) from the internal representation to the larger one.\n",
        "    # So, for example, it can consist of a 256 x 20000 parameter matrix and a 20000 parameter for the bias vector.\n",
        "    projection_layer = Dense(vocab_size, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.1))\n",
        "    \n",
        "    with tf.variable_scope('decod'):\n",
        "        #Training helper used only to read inputs in the embeded stage,As the name indicates, this is only a helper instance.\n",
        "        # This instance should be delivered to the BasicDecoder, which is the actual process of building the decoder model.\n",
        "        train_helper = tf.contrib.seq2seq.TrainingHelper(embeding_matrix, decoder_seq_len)\n",
        "        \n",
        "        #Defining decoder - You can change with BeamSearchDecoder, just beam size\n",
        "        #BasicDecoder builds the decoder model. It means it connects the RNN layer(s) on the decoder side and the input prepared by TrainingHelper\n",
        "        train_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, train_helper, encoder_states,projection_layer)\n",
        "        \n",
        "        #dynamic_decode unrolls the decoder model so that actual prediction can be retrieved by BasicDecoder for each time steps.\n",
        "        train_dec_output, train_dec_state, train_dec_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(train_decoder, \n",
        "                                                                                                          impute_finished=True, \n",
        "                                                                                                          maximum_iterations=max_seq_len)\n",
        "               \n",
        "    #we are using reuse=True option in this scope because we want to get same params learned in the previouse 'decoder' scope    \n",
        "    with tf.variable_scope('decod', reuse=True): \n",
        "        #getting vector of the '<SOS>' tags in the int representation\n",
        "        starting_id_vec = tf.tile(tf.constant([word_to_id['<SOS>']], dtype=tf.int32), [batch_size], name='starting_id_vec')\n",
        "                \n",
        "        #GreedyEmbeddingHelper dynamically takes the output of the current step and give it to the next time step’s input. \n",
        "        #In order to embed the each input result dynamically, embedding parameter(just bunch of weight values) should be provided\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embed_layer, starting_id_vec, word_to_id['<EOS>'])\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,inference_helper, encoder_states, projection_layer)\n",
        "        \n",
        "        inference_dec_output, inference_dec_state, inference_dec_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(inference_decoder, \n",
        "                                                                                                                      impute_finished=True, \n",
        "                                                                                                                      maximum_iterations=max_seq_len)\n",
        "        \n",
        "    return train_dec_output, inference_dec_output\n",
        "\n",
        "\n",
        "def attention_model(rnn_size, keep_probs, encoder_outputs, encoder_states, encoder_seq_len, batch_size):\n",
        "    #rnn_size: int value, The number of units in the LSTM cell.\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
        "    \n",
        "    #An rnn_cell, a projection to output_size is added to it.\n",
        "    #keep_probs: unit Tensor or float between 0 and 1, if it is constant and 1, no input dropout will be added\n",
        "    decoder_cell=tf.contrib.rnn.DropoutWrapper(lstm, keep_probs)\n",
        "    \n",
        "    #using helper function from seq2seq sub_lib for Bahdanau attention\n",
        "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, encoder_outputs, encoder_seq_len)    \n",
        "    \n",
        "    #finishin attention with the attention holder - Attention Wrapper\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, rnn_size/2)\n",
        "    \n",
        "    #Here we are usingg zero_state of the LSTM (in this case) decoder cell, and feed the value of the last encoder_state to it\n",
        "    attention_zero = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
        "    enc_state_new = attention_zero.clone(cell_state=encoder_states[-1])\n",
        "    \n",
        "    return dec_cell, enc_state_new\n",
        "\n",
        "\n",
        "def opt_loss(outputs, targets, dec_seq_len, max_seq_len, learning_rate, clip_rate):\n",
        "   #out put is a predicted value \n",
        "    logits = tf.identity(outputs.rnn_output)\n",
        "    \n",
        "    mask_weigts = tf.sequence_mask(dec_seq_len, max_seq_len, dtype=tf.float32)\n",
        "    \n",
        "    with tf.variable_scope('opt_loss'):\n",
        "        #using sequence_loss to optimize the seq2seq model\n",
        "        loss = tf.contrib.seq2seq.sequence_loss(logits, targets, mask_weigts)                                                       \n",
        "        #Define optimizer\n",
        "        opt = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        #Next 3 lines used to clip gradients {Prevent gradient explosion problem}\n",
        "        gradients = tf.gradients(loss, tf.trainable_variables())\n",
        "        clipped_grads, _ = tf.clip_by_global_norm(gradients, clip_rate)\n",
        "        traiend_opt = opt.apply_gradients(zip(clipped_grads, tf.trainable_variables()))\n",
        "        \n",
        "    return loss, traiend_opt\n",
        "\n",
        "class Chatbot(object):\n",
        "    \n",
        "    def __init__(self,rnn_size,enc_embed_size,dec_embed_size, learning_rate, batch_size, \n",
        "                 number_of_layers, vocab_size, word_to_id, clip_rate):\n",
        "        \n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        self.inputs, self.targets, self.keep_probs, self.encoder_seq_len, self.decoder_seq_len, max_seq_len = graph_inputs()\n",
        "        \n",
        "        \n",
        "        enc_outputs, enc_states = encoder_rnn_layer(self.inputs, rnn_size,number_of_layers,self.encoder_seq_len,\n",
        "                                                    self.keep_probs,enc_embed_size,vocab_size)\n",
        "        \n",
        "        dec_inputs = preprocessing_target(self.targets, word_to_id,batch_size)\n",
        "        \n",
        "        decoder_cell, encoder_states_new = attention_model(rnn_size, self.keep_probs, enc_outputs,enc_states, \n",
        "                                                          self.encoder_seq_len,batch_size)\n",
        "        \n",
        "        train_outputs, inference_output = decoder_rnn_layer(dec_inputs, encoder_states_new,decoder_cell,dec_embed_size,\n",
        "                                                              vocab_size, self.decoder_seq_len,max_seq_len,word_to_id,batch_size)\n",
        "          \n",
        "        self.predictions  = tf.identity(inference_output.sample_id, name='preds')\n",
        "        \n",
        "        self.loss, self.opt = opt_loss(train_outputs, self.targets,self.decoder_seq_len, max_seq_len,learning_rate,clip_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0TVXQ87Symg",
        "colab_type": "text"
      },
      "source": [
        "######### CALCULATE ACCURACY FUNCTION##############"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ctQ2TxPS2QM",
        "colab_type": "code",
        "outputId": "57257773-df8f-407e-95a8-31fd6b46c665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "source": [
        "def accuracy(target, logits):\n",
        "    max_seq_length = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq_length - target.shape[1]:\n",
        "        target = np.pad(target, [(0,0),(0,max_seq_length - target.shape[1])],'constant')\n",
        "    if max_seq_length - logits.shape[1]:\n",
        "        logits = np.pad(logits,[(0,0),(0,max_seq_length - logits.shape[1])],'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))\n",
        "  \n",
        "cleaned_questions, cleaned_answers = clean_data()\n",
        "\n",
        "vocab, word_to_id, id_to_word = create_vocab(cleaned_questions, cleaned_answers)\n",
        "\n",
        "encoded_questions =encoder_word_to_id(cleaned_questions, word_to_id)\n",
        "\n",
        "encoded_answers = encoder_word_to_id(cleaned_answers, word_to_id, True)\n",
        "\n",
        "chatbot_class = Chatbot(config_hp.Rnn_Size,\n",
        "                config_hp.Encod_Embed_Size, \n",
        "                config_hp.Decode_Embed_Size,\n",
        "                config_hp.Learning_Rate, \n",
        "                config_hp.Batch_Size,                  \n",
        "                config_hp.Num_Layers,\n",
        "                len(vocab), \n",
        "                word_to_id, \n",
        "                config_hp.Clip_Rate) \n",
        "\n",
        "# Padding the sequences with the <PAD> token\n",
        "#If the sentence is shorter then wanted length, pad it to that length\n",
        "def apply_padding(batch_of_sequences, word2int):\n",
        "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
        "    return [sequence + [word_to_id['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
        "  \n",
        "# Splitting the data into batches of questions and answers\n",
        "def split_q_a_batches(questions, answers, batch_size):\n",
        "    for batch_index in range(0, len(questions) // batch_size):\n",
        "        start_index = batch_index * batch_size\n",
        "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
        "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
        "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, word_to_id))\n",
        "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, word_to_id))\n",
        "        \n",
        "        yield padded_questions_in_batch, padded_answers_in_batch\n",
        " \n",
        "# Splitting the questions and answers into training and validation sets\n",
        "training_validation_split = int(len(cleaned_questions) * 0.15)\n",
        "training_questions = encoded_questions[training_validation_split:]\n",
        "training_answers = encoded_answers[training_validation_split:]\n",
        "validation_questions = encoded_questions[:training_validation_split]\n",
        "validation_answers = encoded_answers[:training_validation_split]\n",
        "\n",
        "batch_index_check = ((len(training_questions)) // config_hp.Batch_Size // 2) - 1\n",
        "validation_loss = []\n",
        "early_stopping = 1000\t\n",
        "checkpoint = \"chatbot_weights.ckpt\"\n",
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver(max_to_keep=10)\n",
        "for i in range(config_hp.Epochs):    \n",
        "    train_accuracy = []\n",
        "    train_loss = []\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_q_a_batches(training_questions, training_answers, config_hp.Batch_Size)):\n",
        "        starting_time = time.time()\n",
        "        feed_dict = {chatbot_class.inputs:padded_questions_in_batch, \n",
        "                     chatbot_class.targets:padded_answers_in_batch, \n",
        "                     chatbot_class.keep_probs:config_hp.Keep_Prob, \n",
        "                     chatbot_class.decoder_seq_len:[len(padded_answers_in_batch[0])]*config_hp.Batch_Size,\n",
        "                     chatbot_class.encoder_seq_len:[len(padded_answers_in_batch[0])]*config_hp.Batch_Size}\n",
        "        \n",
        "        cost, _, preds = session.run([chatbot_class.loss, chatbot_class.opt, chatbot_class.predictions], feed_dict=feed_dict)\n",
        "            \n",
        "        train_accuracy.append(accuracy(np.array(padded_answers_in_batch), np.array(preds)))      \n",
        "        train_loss.append(cost)\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time        \n",
        "        if batch_index % batch_index_check == 0:\n",
        "          print(\"EPOCH: {}/{}\".format(i, config_hp.Epochs), \n",
        "                \" | Epoch train loss: {}\".format(np.mean(train_loss)), \n",
        "                \" | Epoch train accuracy: {}\".format(np.mean(train_accuracy)),\n",
        "                \" | Batch train time: {}\".format(batch_time))\n",
        "        if batch_index % batch_index_check == 0 and batch_index > 0:\n",
        "            total_validation_loss = []\n",
        "            val_accuracy=[]\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_q_a_batches(validation_questions, validation_answers, config_hp.Batch_Size)):\n",
        "              feed_dict = {chatbot_class.inputs:padded_questions_in_batch, \n",
        "                           chatbot_class.targets:padded_answers_in_batch, \n",
        "                           chatbot_class.keep_probs:1, \n",
        "                           chatbot_class.decoder_seq_len:[len(padded_answers_in_batch[0])]*config_hp.Batch_Size,\n",
        "                           chatbot_class.encoder_seq_len:[len(padded_answers_in_batch[0])]*config_hp.Batch_Size}\n",
        "              \n",
        "              batch_validation_loss,preds = session.run([chatbot_class.loss,chatbot_class.predictions], feed_dict=feed_dict)\n",
        "              \n",
        "              total_validation_loss.append(batch_validation_loss)\n",
        "              val_accuracy.append(accuracy(np.array(padded_answers_in_batch), np.array(preds)))\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss = np.mean(total_validation_loss)\n",
        "            print('Validation Loss: {:>6.3f}, Batch Validation Time: {:d} seconds, Validation Accuracy:{}'.format(average_validation_loss, int(batch_time), np.mean(val_accuracy)))\n",
        "            config_hp.Learning_Rate *= config_hp.Learning_Rate\n",
        "            if config_hp.Learning_Rate < config_hp.Min_Learning_Rate:\n",
        "                learning_rate = config_hp.Min_Learning_Rate\n",
        "            validation_loss.append(average_validation_loss)\n",
        "            if average_validation_loss <= min(validation_loss):\n",
        "                print('I can speak better now!!')\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                if config_hp.Epochs % 10==0:\n",
        "                  saver.save(session, \"/content/drive/My Drive/RNN/checkpoint/chatbot_{}.ckpt\".format(i))\t\n",
        "                  saver.save(session, \"/content/drive/My Drive/RNN/pb/chatbot_{}.pb\".format(i))\n",
        "            else:\n",
        "                print(\"Please train me more, So i will speak better then now .\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping:\n",
        "        print(\"My apologies, This is the best I can do.\")\n",
        "        break\n",
        "print(\"All The best!!\")\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-833c617deb59>:31: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-9-833c617deb59>:38: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-9-833c617deb59>:44: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b64af3ca5006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"chatbot_weights.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_hp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAQbsLKcDy7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}